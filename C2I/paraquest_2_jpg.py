import os
import json
import uuid
import pandas as pd
from PIL import Image
from io import BytesIO
from multiprocessing import Pool, cpu_count
import glob
from multiprocessing import Manager

###  æŠŠimagenet_1kæŒ‰ç…§ç±»åˆ«æ ‡ç­¾åˆ†ç±»å¹¶ä¿å­˜ä¸ºjpg
PARQUET_DIR = "/storage/v-jinpewang/lab_folder/weiming/datasets/imagenet_1k/data"
# è‡ªåŠ¨æ”¶é›†è¯¥ç›®å½•ä¸‹æ‰€æœ‰ parquet æ–‡ä»¶è·¯å¾„
# all_parquet_files = glob.glob(os.path.join(PARQUET_DIR, "*.parquet"))
# PARQUET_FILES = sorted(
#     [p for p in all_parquet_files if "validation-" not in os.path.basename(p)]
# )

PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, "*.parquet")))



OUTPUT_ROOT = "/storage/v-jinpewang/lab_folder/weiming/datasets/imagenet_1k/temp_storage/test_5"
TEMP_JSON_DIR = os.path.join(OUTPUT_ROOT, "temp_jsons")
FINAL_JSON_PATH = os.path.join(OUTPUT_ROOT, "image_label_map.json")
RESIZE_SIZE = 256

DEBUG_MODE = False
DEBUG_NUM = 10  # æ¯ä¸ª parquet ä»…å–å‰ N å¼ 

# NUM_PROCESSES = max(len(PARQUET_FILES), cpu_count()-5)
NUM_PROCESSES = max(1, min(len(PARQUET_FILES), cpu_count() - 2))
MAX_PER_CLASS = 1000          # ğŸ”¹ æ¯ç±»æœ€å¤š50å¼ 
MAX_COMPLETED_CLASSES = 1000 # ğŸ”¹ æ»¡è¶³1000ç±»å°±åœæ­¢æ•´ä¸ªç¨‹åº


def process_parquet(parquet_path, shared_completed_labels, shared_counts, lock):
    """å¤„ç†å•ä¸ª parquet æ–‡ä»¶"""
    if len(shared_completed_labels) >= MAX_COMPLETED_CLASSES:
        print("ğŸš« è¾¾åˆ°ç±»åˆ«ä¸Šé™ï¼Œè·³è¿‡", parquet_path)
        return None

    print(f"ğŸ“¦ å¼€å§‹å¤„ç†: {os.path.basename(parquet_path)}")
    df = pd.read_parquet(parquet_path)

    if DEBUG_MODE:
        df = df.head(DEBUG_NUM)
        print(f"âš™ï¸ è°ƒè¯•æ¨¡å¼å¯ç”¨ï¼Œä»…å¤„ç†å‰ {DEBUG_NUM} å¼ å›¾ç‰‡")

    records = []
    label_counters = {}
    for i, row in df.iterrows():
        # å…ˆå–å‡ºæœ¬è¡Œæ•°æ®
        label_value = row["label"]
        if label_value == -1:   # ğŸš« è·³è¿‡ label == -1 çš„æ ·æœ¬
            continue
        label = str(label_value)
        img_bytes = row["image"]["bytes"]

        # === åŠ é”ï¼šå…¨å±€ç±»æ•°ä¸è¯¥ç±»åé¢â€œé¢„å ä½â€ ===
        with lock:
            # è¾¾åˆ°å…¨å±€1000ç±»å°±æ”¶å·¥
            if len(shared_completed_labels) >= MAX_COMPLETED_CLASSES:
                print("âœ… å·²è¾¾åˆ°1000ä¸ªç±»åˆ«ä¸Šé™ï¼Œæå‰ç»“æŸè¿›ç¨‹")
                break

            # è‹¥è¯¥ç±»å·²å®Œæˆï¼Œç›´æ¥è·³è¿‡
            if label in shared_completed_labels:
                continue

            # å½“å‰å·²ä¿å­˜æ•°ï¼ˆå…¨å±€ï¼‰
            curr = shared_counts.get(label, 0)
            if curr >= MAX_PER_CLASS:
                # ç¬¬ä¸€æ¬¡åˆ°è¾¾500æ—¶ç™»è®°å®Œæˆ
                if label not in shared_completed_labels:
                    shared_completed_labels.append(label)
                    print(f"ğŸ ç±»åˆ« {label} å·²æ”¶é›†æ»¡ {MAX_PER_CLASS} å¼ ï¼Œå…±å®Œæˆ {len(shared_completed_labels)} ç±»")
                continue

            # é¢„å ä¸€ä¸ªåé¢ï¼ˆé˜²æ­¢å¹¶å‘è¶…é¢ï¼‰
            shared_counts[label] = curr + 1
            reserved_to = curr + 1

        # === æ— é”åŒºï¼šå®é™…å†™æ–‡ä»¶ ===
        label_dir = os.path.join(OUTPUT_ROOT, label)
        os.makedirs(label_dir, exist_ok=True)

        img_name = f"{uuid.uuid4()}.jpg"
        img_path = os.path.join(label_dir, img_name)

        try:
            img = Image.open(BytesIO(img_bytes)).convert("RGB")
            # img = img.resize((RESIZE_SIZE, RESIZE_SIZE), Image.BICUBIC)
            img.save(img_path, format="JPEG")
        except Exception as e:
            print(f"[WARN] {parquet_path} ç¬¬ {i} å¼ ä¿å­˜å¤±è´¥: {e}")
            # å›æ»šé¢„å åé¢
            with lock:
                shared_counts[label] = max(0, shared_counts.get(label, 1) - 1)
            continue

        # æˆåŠŸåè®°å½•
        records.append({"image_path": img_path, "label": label})

        # è‹¥æ­£å¥½è¾¾500ï¼Œç™»è®°å®Œæˆï¼ˆåªåœ¨æˆåŠŸä¿å­˜åç™»è®°ï¼‰
        if reserved_to == MAX_PER_CLASS:
            with lock:
                if label not in shared_completed_labels and shared_counts.get(label, 0) >= MAX_PER_CLASS:
                    shared_completed_labels.append(label)
                    print(f"ğŸ ç±»åˆ« {label} å·²æ”¶é›†æ»¡ {MAX_PER_CLASS} å¼ ï¼Œå…±å®Œæˆ {len(shared_completed_labels)} ç±»")

        if i % 100 == 0:
            print(f"{os.path.basename(parquet_path)} å·²å¤„ç† {i}/{len(df)} å¼ å›¾ç‰‡")
    if not records:
        print(f"â„¹ï¸ {parquet_path} æœ¬æ‰¹æ— å¯ä¿å­˜è®°å½•")
        return None
    # å†™å…¥ä¸´æ—¶ JSON æ–‡ä»¶
    os.makedirs(TEMP_JSON_DIR, exist_ok=True)
    temp_json_path = os.path.join(TEMP_JSON_DIR, f"{os.path.basename(parquet_path)}.json")
    
    with open(temp_json_path, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=4)

    print(f"âœ… å®Œæˆ {parquet_path}ï¼Œå…±ä¿å­˜ {len(records)} å¼ å›¾ç‰‡")
    return temp_json_path


if __name__ == "__main__":
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    os.makedirs(TEMP_JSON_DIR, exist_ok=True)

    print(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹ï¼Œè¿›ç¨‹æ•°: {NUM_PROCESSES}")
    manager = Manager()
    shared_completed_labels = manager.list()
    shared_counts = manager.dict() 
    lock = manager.Lock()
    with Pool(processes=NUM_PROCESSES) as pool:
        temp_json_files = pool.starmap(
            process_parquet,
            [(p, shared_completed_labels, shared_counts, lock) for p in PARQUET_FILES]
        )
    # with Pool(processes=NUM_PROCESSES) as pool:
    #     temp_json_files = pool.map(process_parquet, PARQUET_FILES)

    # === åˆå¹¶æ‰€æœ‰ JSON ===
    print("\nğŸ§© æ­£åœ¨åˆå¹¶æ‰€æœ‰ä¸´æ—¶ JSON...")
    merged_records = []
    for path in filter(None, temp_json_files):
        with open(path, "r", encoding="utf-8") as f:
            merged_records.extend(json.load(f))

    print("\nğŸ§¹ æ£€æŸ¥å¹¶ä¿®æ­£æ¯ä¸ªç±»åˆ«å›¾ç‰‡æ•°é‡...")
    from glob import glob
    valid_paths = set()  # å­˜æ”¾ä¿ç•™çš„å›¾ç‰‡è·¯å¾„

    for label in os.listdir(OUTPUT_ROOT):
        label_dir = os.path.join(OUTPUT_ROOT, label)
        if not os.path.isdir(label_dir):
            continue
        imgs = sorted(glob(os.path.join(label_dir, "*.jpg")))
        if len(imgs) > MAX_PER_CLASS:
            extra = imgs[MAX_PER_CLASS:]
            for p in extra:
                os.remove(p)
            print(f"âš–ï¸ {label}: è¶…å‡º {len(imgs) - MAX_PER_CLASS} å¼ ï¼Œå·²åˆ é™¤å¤šä½™å›¾ç‰‡")
            imgs = imgs[:MAX_PER_CLASS]
        for p in imgs:
            valid_paths.add(os.path.abspath(p))

    # è¿‡æ»¤ JSON ä¸­æ— æ•ˆå›¾ç‰‡è·¯å¾„
    before = len(merged_records)
    merged_records = [rec for rec in merged_records if os.path.abspath(rec["image_path"]) in valid_paths]
    after = len(merged_records)
    print(f"ğŸ§¾ JSON ä¿®æ­£å®Œæˆï¼Œç§»é™¤ {before - after} æ¡æ— æ•ˆè®°å½•")
        # === æ£€æŸ¥å¹¶ä¿®æ­£è¶…é™ç±»åˆ« ===
    from collections import Counter
    from glob import glob

    label_counts = Counter([rec["label"] for rec in merged_records])
    over_labels = {k: v for k, v in label_counts.items() if v > MAX_PER_CLASS}

    if over_labels:
        print("\nğŸš¨ æ£€æµ‹åˆ°ä»¥ä¸‹ç±»åˆ«è¶…è¿‡é™åˆ¶ï¼Œå¼€å§‹ä¿®æ­£...")
        valid_paths = set(p["image_path"] for p in merged_records)  # å½“å‰JSONä¸­çš„æ‰€æœ‰è·¯å¾„
        removed_paths = set()

        for label, count in sorted(over_labels.items(), key=lambda x: -x[1]):
            label_dir = os.path.join(OUTPUT_ROOT, label)
            imgs = sorted(glob(os.path.join(label_dir, "*.jpg")))
            if len(imgs) > MAX_PER_CLASS:
                extra = imgs[MAX_PER_CLASS:]
                for p in extra:
                    try:
                        os.remove(p)
                        removed_paths.add(os.path.abspath(p))
                    except Exception as e:
                        print(f"[WARN] åˆ é™¤ {p} å¤±è´¥: {e}")
                print(f"âš–ï¸ {label}: å·²åˆ é™¤ {len(extra)} å¼ å¤šä½™å›¾ç‰‡")

        # åŒæ­¥ä¿®æ­£ JSON
        before = len(merged_records)
        merged_records = [rec for rec in merged_records if os.path.abspath(rec["image_path"]) not in removed_paths]
        after = len(merged_records)
        print(f"âœ… å·²ç§»é™¤ {before - after} æ¡è¶…é™è®°å½•")
    else:
        print("\nâœ… æ²¡æœ‰ç±»åˆ«è¶…è¿‡ MAX_PER_CLASS é™åˆ¶")
    # === å†™å‡ºæœ€ç»ˆ JSON ===
    with open(FINAL_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(merged_records, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å…±ä¿å­˜ {len(merged_records)} å¼ å›¾ç‰‡")
    print(f"ğŸ‘‰ æœ€ç»ˆ JSON æ–‡ä»¶è·¯å¾„: {FINAL_JSON_PATH}")
